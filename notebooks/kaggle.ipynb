{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install lightning\n",
    "!pip install --force-reinstall tensorflow-io"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-18T22:50:14.275198Z",
     "iopub.execute_input": "2023-07-18T22:50:14.275806Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"xlm-roberta-base\", cache_dir=os.getenv(\"CACHE_DIR\"))\n",
    "\n",
    "# Align label with subtokens generated through tokenizer\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = TOKENIZER(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "'''\n",
    "taken from the SPLICER repository: Label indices larger than 6 are remapped to 0.\n",
    "'''\n",
    "def remap_to_wikiann_labels(examples: dict) -> dict:\n",
    "    examples[\"ner_tags\"] = [\n",
    "        [tag if tag < 7 else 0 for tag in instance] for instance in examples[\"ner_tags\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "class MonolingualNERDataSet:\n",
    "    def __init__(self,\n",
    "                 name=\"conll2003\",\n",
    "                 split=\"train\",\n",
    "                 languages=None) -> None:\n",
    "        super().__init__()\n",
    "        if languages:\n",
    "            self.dataset = datasets.concatenate_datasets(list(load_dataset(name, lang, cache_dir=os.getenv(\"CACHE_DIR\"), split=split) for lang in languages))\n",
    "        else:\n",
    "            self.dataset = load_dataset(name, cache_dir=os.getenv(\"CACHE_DIR\"), split=split)\n",
    "        self.dataset_name = name\n",
    "        self.tokenized_datasets = self.dataset.map(remap_to_wikiann_labels,batched=True)\n",
    "        self.tokenized_datasets = self.tokenized_datasets.map(tokenize_and_align_labels, batched=True).map(batched=True, remove_columns=self.dataset.column_names)\n",
    "        self.tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\", \"labels\"])\n",
    "\n",
    "class MultilingualNERDataSet:\n",
    "    def __init__(self,\n",
    "                 name=\"masakhaner\",\n",
    "                 languages=[\"amh\", \"hau\", \"ibo\", \"kin\", \"lug\", \"luo\", \"pcm\", \"swa\", \"wol\", \"yor\"],\n",
    "                 split=\"test\") -> None:\n",
    "        super().__init__()\n",
    "        self.dataset_names = languages\n",
    "        self.datasets = list(load_dataset(name, lang, split=split, cache_dir=os.getenv(\"CACHE_DIR\")) for lang in languages)\n",
    "        self.tokenized_datasets = [dataset.map(remap_to_wikiann_labels,batched=True).map(tokenize_and_align_labels, batched=True).map(batched=True, remove_columns=self.datasets[0].column_names) for dataset in self.datasets]\n",
    "        for dataset in self.tokenized_datasets:\n",
    "            dataset.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\", \"labels\"])\n",
    "\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "_ENGLISH = [\n",
    "    \"en\"\n",
    "]\n",
    "\n",
    "_MASAKHANER_LANGS = [\"amh\", \"hau\", \"ibo\", \"kin\", \"lug\", \"luo\", \"pcm\", \"swa\", \"wol\", \"yor\"]\n",
    "\n",
    "class SlicerLightningDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=16, num_workers=2, tokenizer_checkpoint=\"xlm-roberta-base\"):\n",
    "        super().__init__()\n",
    "        self.model_checkpoint = tokenizer_checkpoint\n",
    "        self.batch_size = batch_size\n",
    "        self.test_datasets_names  = None\n",
    "        self.data_collator = None\n",
    "        self.tokenizer = None\n",
    "        self.num_workers = num_workers\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage=\"train_conll\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)\n",
    "        self.data_collator = DataCollatorForTokenClassification(tokenizer=self.tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "        if stage == \"train_conll\":\n",
    "            # Load data for training\n",
    "            dataset = MonolingualNERDataSet(name=\"conll2003\", split=\"train\")\n",
    "            self.train_dataset = dataset.tokenized_datasets\n",
    "\n",
    "            # Load data for validation\n",
    "            datasets = MonolingualNERDataSet(name=\"conll2003\", split=\"validation\")\n",
    "            self.validation_dataset = datasets.tokenized_datasets        \n",
    "\n",
    "        if stage == \"train_wikiann\":\n",
    "            # Load data for training\n",
    "            dataset = MonolingualNERDataSet(name=\"wikiann\", split=\"train\", languages=_ENGLISH)\n",
    "            self.train_dataset = dataset.tokenized_datasets\n",
    "\n",
    "            # Load data for validation\n",
    "            datasets = MonolingualNERDataSet(name=\"wikiann\", split=\"validation\", languages=_ENGLISH)\n",
    "            self.validation_dataset = datasets.tokenized_datasets\n",
    "\n",
    "        masakhaner = MultilingualNERDataSet(\"masakhaner\", _MASAKHANER_LANGS, split=\"test\")\n",
    "        self.test_datasets_names = masakhaner.dataset_names\n",
    "        self.test_datasets = masakhaner.tokenized_datasets\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True, \n",
    "            collate_fn=self.data_collator, \n",
    "            # num_workers=self.num_workers\n",
    "            )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.validation_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False, \n",
    "            collate_fn=self.data_collator,\n",
    "            # num_workers=self.num_workers\n",
    "            )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataloaders = {}\n",
    "        for index in range(len(self.test_datasets_names)):\n",
    "            dataloader = DataLoader(\n",
    "                self.test_datasets[index], \n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=self.data_collator)\n",
    "            name = self.test_datasets_names[index]\n",
    "            dataloaders[name] = dataloader\n",
    "        return dataloaders\n",
    "    # HINT: Evaluating on multiple dataloaders (https://lightning.ai/docs/pytorch/LTS/guides/data.html)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class SlicerRobertaNER(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_labels = 9,\n",
    "                 slice_size = 4,\n",
    "                 hidden_size = 768,\n",
    "                 num_slices = 192):\n",
    "        # Save HP to checkpoints\n",
    "        super().__init__()\n",
    "        self.n_labels = n_labels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.slice_size = slice_size\n",
    "        self.num_slices = num_slices\n",
    "\n",
    "        # Init model\n",
    "        self.base = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "        linear = nn.Linear(self.hidden_size, self.n_labels)\n",
    "        \n",
    "        classifier_weights = linear.weight.data.T.reshape((self.num_slices, self.slice_size, self.n_labels))\n",
    "        classifier_bias = linear.bias\n",
    "        \n",
    "        self.classifier_weights = torch.nn.Parameter(classifier_weights, requires_grad=True)\n",
    "        self.classifier_bias = torch.nn.Parameter(classifier_bias, requires_grad=True)\n",
    "\n",
    "        #self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        classified_outputs = self.base(**x)\n",
    "        logits = classified_outputs[0]\n",
    "\n",
    "        ####\n",
    "        batch_size, sequence_length, hidden = logits.shape\n",
    "        ####\n",
    "\n",
    "        ####\n",
    "        num_slices, slice_size, n_labels = self.classifier_weights.shape\n",
    "        ####\n",
    "\n",
    "        #we slice along the hidden dimension in num_slices slides\n",
    "        sliced_outputs = logits.reshape((batch_size * sequence_length, self.num_slices, self.slice_size))\n",
    "        #sliced_classifier= weight_matrix.reshape((self.num_slices, self.slice_size, self.n_labels)).to(sliced_outputs.device)\n",
    "        self.classifier_weights = self.classifier_weights.to(sliced_outputs.device)\n",
    "\n",
    "        #and combine them:\n",
    "        #target shape\n",
    "        #classified_outputs = torch.zeros((batch_size*sequence_length, self.num_slices, self.n_labels)).to(sliced_outputs.device)\n",
    "        #combination\n",
    "        #for i in range(batch_size*sequence_length):\n",
    "        #    for j in range(self.num_slices):\n",
    "        #        for kk in range(self.slice_size):\n",
    "        #            for l in range(self.n_labels):\n",
    "        #                classified_outputs[i, j, l] += sliced_outputs[i, j, kk] * sliced_classifier[j, kk, l]\n",
    "        \n",
    "        classified_outputs = torch.einsum(\"ndk, dkl->ndl\", sliced_outputs, self.classifier_weights).reshape((-1, self.n_labels)) + self.classifier_bias\n",
    "        \n",
    "    \n",
    "        # reshape to be compatible with later operations\n",
    "\n",
    "        return classified_outputs\n",
    "\n",
    "\n",
    "\n",
    "class RobertaNERAdvanced(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_labels=9,\n",
    "                 hidden_size=384):\n",
    "        # Save HP to checkpoints\n",
    "        super().__init__()\n",
    "        self.n_labels = n_labels\n",
    "        self.hidden_size = hidden_size\n",
    "        # Init model\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "        self.dense = nn.Linear(768, hidden_size)\n",
    "        # self.dropout = nn.Dropout(p=0.2)\n",
    "        self.classification_head = nn.Linear(hidden_size, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.base(**x)\n",
    "        logits = output[0]\n",
    "        logits = self.dense(logits)\n",
    "        # logits = self.dropout(logits)\n",
    "        logits = self.classification_head(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class RobertaNER(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_labels=9):\n",
    "        # Save HP to checkpoints\n",
    "        super().__init__()\n",
    "        self.n_labels = n_labels\n",
    "        # Init model\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "        self.classification_head = nn.Linear(768, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.base(**x)\n",
    "        logits = output[0]\n",
    "        logits = self.classification_head(logits)\n",
    "        return logits"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics.functional\n",
    "from torch import argmax\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.functional import cross_entropy\n",
    "from transformers import AutoModel\n",
    "from torchmetrics.functional.classification import f1_score, accuracy, precision, recall\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "\n",
    "label_dict = {0: 'O', 1:  'B-PER', 2: 'I-PER',3: 'B-ORG',4: 'I-ORG',5: 'B-LOC', 6: 'I-LOC',7: 'B-MISC',8: 'I-MISC'}\n",
    "def lookup_table(label):\n",
    "    return label_dict[label]\n",
    "\n",
    "class SlicerLightningModule(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                test_datasets_names,\n",
    "                 n_labels = 7,\n",
    "                 hidden_size = 768, \n",
    "                 learning_rate = 2e-5, \n",
    "                 weight_decay = 0.05,\n",
    "                 is_slicer = True,\n",
    "                 slice_size = 4\n",
    "                 ):\n",
    "        # Save HP to checkpoints\n",
    "        super().__init__()\n",
    "        self.n_labels = n_labels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weigt_decay = weight_decay\n",
    "        self.is_slicer = is_slicer\n",
    "        self.slice_size = slice_size\n",
    "        self.num_slices = int(self.hidden_size/self.slice_size)\n",
    "        self.test_datasets_names  = test_datasets_names\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Init model\n",
    "        if (self.is_slicer):\n",
    "            self.model = SlicerRobertaNER(n_labels=self.n_labels, slice_size=slice_size, num_slices=self.num_slices)\n",
    "        else:\n",
    "            self.model = RobertaNER(n_labels=self.n_labels)\n",
    "\n",
    "\n",
    "    def __default_step(self, batch, batch_idx):\n",
    "        x, labels = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}, batch['labels']\n",
    "\n",
    "        logits = self.model(x).reshape((-1, self.n_labels))  # combine batch and sequence (and num_slices) into one dim\n",
    "        labels = labels.to(logits.device).reshape((-1,))  # combine batch and sequence into one dim\n",
    "\n",
    "        # length of logits is different depending on SLICER/STANDARD\n",
    "        if (self.is_slicer):\n",
    "            labels = labels.repeat_interleave(self.num_slices)\n",
    "\n",
    "        loss = cross_entropy(logits, labels, ignore_index=-100)\n",
    "\n",
    "        return logits, labels, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _,_, loss = self.__default_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits, labels, loss = self.__default_step(batch, batch_idx)\n",
    "        micro_f1, f1, precision, recall, accuracy = self.computeMetrics(logits, labels)\n",
    "        \n",
    "        self.log(\"val_f1\", micro_f1, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.logMetrics(loss, f1, recall, precision, accuracy, \"val\")\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx):\n",
    "\n",
    "        logits, labels, loss = self.__default_step(batch, batch_idx)\n",
    "        micro_f1, f1, precision, recall, accuracy = self.computeMetrics(logits, labels)\n",
    "\n",
    "        dataset_name = self.test_datasets_names[dataloader_idx]\n",
    "        self.log(f\"{dataset_name}_test_f1\", micro_f1, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.logMetrics(loss, f1, recall, precision, accuracy, f\"{dataset_name}_test\")\n",
    "\n",
    "\n",
    "    def computeMetrics(self, logits, labels):\n",
    "        # 7-er vector giving scores for each class individually\n",
    "        micro_f1 = f1_score(preds=logits, target=labels,\n",
    "                                 task='multiclass', num_classes=self.n_labels, ignore_index=-100, average='micro', multidim_average='global')\n",
    "        f1 = f1_score(preds=logits, target=labels,\n",
    "                           task='multiclass', num_classes=self.n_labels, ignore_index=-100, average='none', multidim_average='global')\n",
    "        p = precision(preds=logits, target=labels,\n",
    "                                   task='multiclass', num_classes=self.n_labels, ignore_index=-100, average='none', multidim_average='global')\n",
    "        r = recall(preds=logits, target=labels,\n",
    "                             task='multiclass', num_classes=self.n_labels, ignore_index=-100, average='none',multidim_average='global')\n",
    "        a = accuracy(preds=logits, target=labels,\n",
    "                                 task='multiclass', num_classes=self.n_labels, ignore_index=-100, average='none',multidim_average='global')\n",
    "\n",
    "        return micro_f1, f1, p, r, a\n",
    "\n",
    "    def logMetrics(self, loss, f1, recall, precision, accuracy, stage : str):\n",
    "        #9er vector giving scores for each class individually\n",
    "        dict = {f\"{stage}_loss\": loss}\n",
    "        for i in range(self.n_labels):\n",
    "            dict[f\"{lookup_table(i)}_{stage}_recall\"] = recall[i]\n",
    "            dict[f\"{lookup_table(i)}_{stage}_precision\"] = precision[i]\n",
    "            dict[f\"{lookup_table(i)}_{stage}_f1\"] = f1[i]\n",
    "            dict[f\"{lookup_table(i)}_{stage}_accuracy\"] = accuracy[i]\n",
    "        self.log_dict(dict, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "\n",
    "\n",
    "    # def predict_step(self, batch, batch_idx):\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), \n",
    "                                lr=self.learning_rate, \n",
    "                                weight_decay=self.weigt_decay)\n",
    "\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import wandb\n",
    "from lightning import seed_everything\n",
    "import torch\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "\n",
    "\n",
    "def create_experiment_name(args):\n",
    "    base_name = 'Slicer' if args.slicer else 'Vanilla'\n",
    "    dataset = str(args.dataset)\n",
    "    batch_size = str(args.batch_size)\n",
    "    num_slices = str(args.slice_size)\n",
    "    return 'NER_' + base_name \\\n",
    "        + \"-Dataset\" + dataset \\\n",
    "        + \"-BatchSize\" + batch_size \\\n",
    "        + \"-Slices\" + num_slices \\\n",
    "\n",
    "def main(hparams):\n",
    "    # Random seeding for reproducablity\n",
    "    seed_everything(seed=hparams.random_seed, workers=True)\n",
    "\n",
    "    # float precision\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    # Init logging\n",
    "    experiment_name = create_experiment_name(hparams)\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"MLNLP_Slicer\", \n",
    "        name=experiment_name,\n",
    "        checkpoint_name= experiment_name,\n",
    "        offline=hparams.offline,\n",
    "        log_model=True\n",
    "        )\n",
    "\n",
    "    datamodule = SlicerLightningDataModule(        \n",
    "        batch_size=hparams.batch_size, \n",
    "        num_workers = hparams.num_workers)\n",
    "    \n",
    "    # important to call here, otherwise, the name of the test datasets will not be set causing erors in the module\n",
    "    datamodule.setup(stage=\"train_\"+hparams.dataset)\n",
    "\n",
    "    # Init the building blocks\n",
    "    module = SlicerLightningModule(\n",
    "        test_datasets_names=datamodule.test_datasets_names,\n",
    "        n_labels=7,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.05,\n",
    "        is_slicer=hparams.slicer,\n",
    "        slice_size= hparams.slice_size\n",
    "    )\n",
    "\n",
    "    # Trainer (https://lightning.ai/docs/pytorch/stable/common/trainer.html)\n",
    "    trainer = pl.Trainer(accelerator=hparams.accelerator,\n",
    "                         max_epochs=10 if hparams.dataset == \"conll\" else 5,\n",
    "                         accumulate_grad_batches=8,\n",
    "                         logger=wandb_logger,\n",
    "                         enable_progress_bar=True,\n",
    "                         fast_dev_run=hparams.fast_dev_run,\n",
    "                         deterministic=hparams.deterministic,\n",
    "                         )\n",
    "\n",
    "    # Fit the model (and evaluate on validation data as defined)\n",
    "    trainer.fit(module, datamodule=datamodule)\n",
    "    # trainer.save_checkpoint(\"./../checkpoints/\" + experiment_name + \"/example.ckpt\")\n",
    "\n",
    "    # Test model\n",
    "    if not hparams.fast_dev_run:\n",
    "        trainer.test(datamodule=datamodule, ckpt_path='last')\n",
    "\n",
    "    del module\n",
    "    del datamodule\n",
    "    del trainer\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    wandb_logger.experiment.finish()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.login(key='a49faeab1679e732df6f7e70378fea80f88c48a2')\n",
    "    \n",
    "    search_space = {\n",
    "        \"dataset\": [\"wikiann\"], \n",
    "        \"slice_size\": [1, 2, 8]\n",
    "    }\n",
    "                    \n",
    "    for dataset in search_space[\"dataset\"]:        \n",
    "            for slice in search_space[\"slice_size\"]:      \n",
    "                hparams = Namespace()\n",
    "                hparams.accelerator = \"gpu\"\n",
    "                hparams.batch_size = 4\n",
    "                hparams.dataset = dataset\n",
    "                hparams.deterministic = True\n",
    "                hparams.fast_dev_run = False\n",
    "                hparams.slice_size = slice\n",
    "                hparams.offline = False\n",
    "                hparams.random_seed = 42\n",
    "                hparams.num_workers = 4   \n",
    "                hparams.slicer = True\n",
    "                    \n",
    "                    \n",
    "                main(hparams)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
